{
  "model_version": {
    "name": "marketplace_staging_llama_2_models.models.llama_2_7b_chat_hf",
    "version": "1",
    "creation_timestamp": 1696530586359,
    "last_updated_timestamp": 1696530586359,
    "user_id": "first.last@mycompany.com",
    "description": "The `llama_2_7b_chat_hf` model version 1 is packaged at revision\n[08751db2aca9bf2f7f80d2e516117a53d7450235](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commit/08751db2aca9bf2f7f80d2e516117a53d7450235). Refer to\nthe [MetaAI Llama 2 page](https://ai.meta.com/llama/) for\nfull details around limitations, evaluated use, and broader implications.\nFor usage of the model on Databricks,\nreview the description of the model in the model overview.\n\n[Llama 2 is licensed under the LLAMA 2 Community License, Copyright \u00a9 Meta Platforms, Inc. All Rights Reserved](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n",
    "source": "",
    "status": "READY",
    "storage_location": "r2://marketplace-staging@c48df378c07a2e713d4d61c040341a8a.r2.cloudflarestorage.com/models/7d608abb-48db-4cc8-be1b-ff7751579000/versions/1db4499f-e035-4581-898c-8751ccdc08d7",
    "_creation_timestamp": "2023-10-05 18:29:46",
    "_last_updated_timestamp": "2023-10-05 18:29:46",
    "_is_unity_catalog": true,
    "_reg_model_download_uri": "r2://marketplace-staging@c48df378c07a2e713d4d61c040341a8a.r2.cloudflarestorage.com/models/7d608abb-48db-4cc8-be1b-ff7751579000/versions/1db4499f-e035-4581-898c-8751ccdc08d7",
    "_run_model_download_uri": "",
    "_web_ui_link": "https://v9-antfood.mycompany.com/explore/data/models/marketplace_staging_llama_2_models.models.llama_2_7b_chat_hf/version/1",
    "_api_link": "https://v9-antfood.mycompany.com/api/2.0/mlflow/unity-catalog/model-versions/get?name=marketplace_staging_llama_2_models.models.llama_2_7b_chat_hf&version=1"
  },
  "mlflow_model": {
    "error": {
      "message": "Cannot download artifact 'models:/marketplace_staging_llama_2_models.models.llama_2_7b_chat_hf/1/MLmodel'",
      "exception_type": "<class 'botocore.exceptions.ClientError'>",
      "exception": "An error occurred (InvalidRegionName) when calling the GetBucketLocation operation: The region name 'us-west-2' is not valid. Must be one of: wnam, enam, weur, eeur, apac, auto"
    }
  },
  "registered_model": {
    "name": "marketplace_staging_llama_2_models.models.llama_2_7b_chat_hf",
    "creation_timestamp": 1695830136104,
    "last_updated_timestamp": 1701469963139,
    "user_id": "first.last@mycompany.com",
    "description": "\nllama_2_7b_chat_hf - **preview**\n================================\n\n\nThe `llama_2_7b_chat_hf` model offered in Databricks Marketplace is a text-generation model released by Meta AI. It is an [MLflow](https://mlflow.org/docs/latest/index.html) model that packages\n[Hugging Face\u2019s implementation for the llama_2_7b_chat_hf model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\nusing the [transformers](https://mlflow.org/docs/latest/models.html#transformers-transformers-experimental) \nflavor in MLflow.\n\n- It has 7 billion parameters. While it offers the fastest processing speed, it may have lower quality compared to other models in the model family.\n- It is fine-tuned specifically for dialogue use cases.\n\n\n**Input:** string containing the text of instructions\n\n**Output:** string containing the generated response text\n\nFor example notebooks of using the `llama_2_7b_chat_hf` model in various use cases on Databricks, refer to [the Databricks ML example repository](https://github.com/databricks/databricks-ml-examples/tree/master/llm-models/llamav2/llamav2-7b). For details about the `llama_2_7b_chat_hf` model, please visit [the Hugging Face model card](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n\n# Evaluation results\n\nThe evaluation results for the `llama_2_7b_chat_hf` model, as measured by the [Mosaic Eval Gauntlet](https://www.mosaicml.com/llm-evaluation) framework, are presented below. This framework comprises a series of tasks specifically designed to assess the performance of language models. It includes datasets from MMLU, Big-Bench, HellaSwag, among others.\n\n- **Core Average**: 0.42\n- **World Knowledge**: 0.476\n- **Commonsense Reasoning**: 0.447\n- **Language Understanding**: 0.478\n- **Symbolic Problem Solving**: 0.221\n- **Reading Comprehension**: 0.478\n\n\nRefer to the [Mosaic Eval Gauntlet blog post](https://www.mosaicml.com/llm-evaluation) for the benchmarks included in each category.\nFor a leaderboard of more models, refer to the [example repo](https://github.com/databricks/databricks-ml-examples).\n\n# Usage\n\nDatabricks recommends that you primarily work with this model via Model Serving ([AWS](https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/create-manage-serving-endpoints)).\n\nNote: Model serving is not supported on GCP. On GCP, Databricks recommends running `Batch inference using Spark`, \nas shown below or check the example in [example repo](https://github.com/databricks/databricks-ml-examples/tree/master/llm-models/llamav2/llamav2-7b/08_load_from_marketplace.py).\n\n## Deploying the model to Model Serving\n\nYou can deploy this model directly to a Databricks Model Serving Endpoint ([AWS](https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/create-manage-serving-endpoints)).\nWe recommend using the `GPU_MEDIUM` workload type on AWS and `GPU_LARGE` workload type on Azure for this model.\nThe model will automatically be served using Optimized LLM serving ([AWS](https://docs.databricks.com/en/machine-learning/model-serving/llm-optimized-model-serving.html#input-and-output-schema-format)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/llm-optimized-model-serving)), for improved throughput and latency.\n\nTo create the endpoint, click the \u201cServe this model\u201d button above or use the REST API as follows:\n\n```\nPOST /api/2.0/serving-endpoints\n{\n  \"name\": \"llama_2_7b_chat_hf\",\n  \"config\":{\n      \"served_models\": [{\n          \"model_name\": \"<UC LOCATION FOR MODEL>\",\n          \"model_version\": \"2\",\n          \"workload_size\": \"Small\",\n          \"workload_type\": \"<workload_type>\",\n          \"scale_to_zero_enabled\": false\n      }],\n  }\n}\n```\n\nRefer to Databricks documentation regarding REST API authentication ([AWS](https://docs.databricks.com/dev-tools/auth.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/auth)) and access control for Model Serving ([AWS](https://docs.databricks.com/security/auth-authz/access-control/serving-endpoint-acl.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/serving-endpoint-acl)). Additionally, note that optimized LLM serving endpoints have input and output schemas that Databricks controls. See docs ([AWS](https://docs.databricks.com/en/machine-learning/model-serving/llm-optimized-model-serving.html#input-and-output-schema-format)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/model-serving/llm-optimized-model-serving)) for details.\n\n## SQL transcription using ai_query\n\nTo generate the text using the endpoint, use `ai_query` ([AWS](https://docs.databricks.com/sql/language-manual/functions/ai_query.html)|[Azure](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/ai_query)) to query the Model Serving endpoint.\nThe first parameter should be the name of the endpoint you previously created for Model Serving. The second parameter should be a `named_struct` with name `prompt` and value is the column name that containing the instruction text. Extra parameters can be added to the named_struct too. For supported parameters, please refer to [MLFlow AI gateway completion routes](https://mlflow.org/docs/latest/llms/gateway/index.html#completions). The third and fourth parameters set the return type, so that `ai_query` can properly parse and structure the output text.\n\n```sql\nSELECT \nai_query(\n  'llama_2_7b_chat_hf',\n  named_struct(\"prompt\", text,  \"max_tokens\", 256),\n  'returnType',\n  'STRUCT<candidates:ARRAY<STRUCT<text:STRING, metadata:STRUCT<finish_reason:STRING>>>, metadata:STRUCT<input_tokens:float, output_tokens:float, total_tokens:float> >'\n) as generated_text\nfrom <TABLE>\n```\n\nYou can use `ai_query` in this manner to generate text in SQL queries or notebooks connected to Databricks SQL Pro or Serverless SQL Endpoints.\n\n## Generate the text by querying the serving endpoint\n\n\nTo query the model serving endpoint, first install the newest Databricks SDK for Python.\n\n```python\n# Upgrade to use the newest Databricks SDK\n%pip install --upgrade databricks-sdk\ndbutils.library.restartPython()\n```\n\nWith the newest Databricks SDK installed, query the serving endpoint as follows:\n\n```python\n# Run below in a separate notebook cell\nfrom databricks.sdk import WorkspaceClient\n\n# Change it to your own input\ndataframe_records = [\n    {\"prompt\": \"<INSTRUCTION TEXT>\", \"max_tokens\": 512}\n]\n\nw = WorkspaceClient()\nw.serving_endpoints.query(\n    name=\"llama_2_7b_chat_hf\",\n    dataframe_records=dataframe_records,\n)\n```\n## Batch inference using Spark\n\n\nYou can also directly load the model as a Spark UDF and run batch inference on Databricks compute using Spark.\n\nDatabricks recommends using a GPU cluster with\n- Databricks Runtime for Machine Learning version 14.2 or greater\n- Recommended compute size: 1xA10\n- Recommended instance type:\n    - GCP: `g2-standard-4`\n    - AWS: `g5.8xlarge`\n    - Azure: `Standard_NV36ads_A10_v5`\n\n```python\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\n\ncatalog_name = \"databricks_llama_2_models\"\ngenerate = mlflow.pyfunc.spark_udf(spark, f\"models:/{catalog_name}.models.llama_2_7b_chat_hf/2\", \"string\")\n```\n\nYou can use the UDF directly on text column:\n\n```python\nimport pandas as pd\ndf = spark.createDataFrame(pd.DataFrame({\"text\":pd.Series(\"<INSTRUCTION TEXT>\")}))\ngenerated_df = df.select(generate(df.text).alias('generated_text'))\n```",
    "tags": {},
    "_creation_timestamp": "2023-09-27 15:55:36",
    "_last_updated_timestamp": "2023-12-01 22:32:43",
    "_is_unity_catalog": true,
    "_web_ui_link": "https://v9-antfood.mycompany.com/explore/data/models/marketplace_staging_llama_2_models.models.llama_2_7b_chat_hf",
    "_api_link": "https://v9-antfood.mycompany.com/api/2.0/mlflow/unity-catalog/registered-models/get?name=marketplace_staging_llama_2_models.models.llama_2_7b_chat_hf"
  },
  "run": {
    "warning": "Model version 'marketplace_staging_llama_2_models.models.llama_2_7b_chat_hf/1' has no run_id"
  }
}
